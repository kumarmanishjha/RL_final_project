{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-rsljIr7Byd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it_BlHb27Gii",
        "colab_type": "code",
        "outputId": "2f272d48-756f-4a22-bdfc-9aa7ae388c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, sys, time, datetime, random\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Activation\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.activations import relu\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F4FgLVc7KnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visited cells\n",
        "visited_mark = 0.9\n",
        "target_mark = 0.5\n",
        "human_mark = 0.2\n",
        "\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict = {\n",
        "    0: 'left',\n",
        "    1: 'up',\n",
        "    2: 'right',\n",
        "    3: 'down',\n",
        "    4: 'do nothing'\n",
        "}\n",
        "\n",
        "num_actions = len(actions_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZrdP5UZ7Mj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Maze:\n",
        "    def __init__(self, maze, target_list, human = (0, 0), hint = 4):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self._targets = target_list\n",
        "        self.terminal_cell = self._maze[nrows-1, ncols-1]\n",
        "        self.free_cells = self.get_free_cells()\n",
        "        self.free_cells.discard(self.terminal_cell)\n",
        "        self.blocked_cells = self.get_blocked_cells()\n",
        "        self.reset(human, hint)\n",
        "        \n",
        "    def reset(self, human = (0,0),  hint= 4):\n",
        "        self.hint = hint\n",
        "        self.human = human\n",
        "        self.maze = np.copy(self._maze)\n",
        "        self.targets = set(self._targets)\n",
        "        #print(self.targets)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        row, col = human\n",
        "        self.maze[row, col] = human_mark\n",
        "        self.state = ((row, col), 'start')\n",
        "        self.base = np.sqrt(self.maze.size)\n",
        "        self.visited = self.visited = dict(((r,c),0) for r in range(nrows) \n",
        "                                           for c in range(ncols) if self._maze[r,c] == 1.0)\n",
        "        self.total_reward = 0\n",
        "        self.min_reward =  -0.4 * self.maze.size\n",
        "        self.reward = {\n",
        "            'blocked'      :  self.min_reward,\n",
        "            'targets'      :  1.0/len(self._targets),\n",
        "            'invalid'      : -4.0/self.base,\n",
        "            'valid'        : -1/self.maze.size, #(1 + 0.1*self.visited[agent] ** 2)\n",
        "            'hint_penalty' : -2/self.maze.size\n",
        "        }\n",
        "    \n",
        "    def get_visited_cells(self):\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        visited = dict(((r,c),0) \n",
        "             for r in range(self.maze.nrows) \n",
        "             for c in range(self.maze.ncols) if self._maze[r,c] == 1.0)\n",
        "        return visited\n",
        "    \n",
        "    def valid_actions(self, cell = None):\n",
        "        if cell is None:\n",
        "            row, col = self.human\n",
        "        else:\n",
        "            row, col = cell\n",
        "        nrows, ncols = self.maze.shape\n",
        "        actions = [0, 1, 2, 3]\n",
        "        \n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(3)\n",
        "            \n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(2)\n",
        "        \n",
        "        if row > 0 and self.maze[row-1, col] == 0:\n",
        "            actions.remove(1)\n",
        "        if row < nrows-1 and self.maze[row+1, col] == 0:\n",
        "            actions.remove(3)\n",
        "            \n",
        "        if col>0 and self.maze[row, col-1] == 0:\n",
        "            actions.remove(0)\n",
        "        if col<ncols-1 and self.maze[row, col+1] ==0:\n",
        "            actions.remove(2) \n",
        "            \n",
        "        return actions\n",
        "        \n",
        "    def get_action(self, hint = 4):\n",
        "        #Get next action based on hint\n",
        "        if hint == 4:\n",
        "            action = random.choice(self.valid_actions())\n",
        "        else:\n",
        "            action = hint\n",
        "            \n",
        "        return action\n",
        "    \n",
        "    def act(self, hint, action):\n",
        "        #Take action, update state and reward\n",
        "        #print(hint, action)\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward(hint)\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        env_state = self.observe()\n",
        "        return env_state, reward, status\n",
        "    \n",
        "    def get_reward(self, hint = 4):\n",
        "        #\n",
        "        human, mode = self.state\n",
        "        penalty = 0\n",
        "        if hint != 4:\n",
        "            penalty =  self.reward['hint_penalty']\n",
        "            \n",
        "        if np.all(human == self.terminal_cell) and len(self.targets) == 0:\n",
        "            return penalty + 1.0\n",
        "        elif human in self.targets:\n",
        "            return penalty + self.reward['targets']\n",
        "        \n",
        "        if mode =='blocked':\n",
        "            return penalty + self.reward['blocked']\n",
        "        elif mode == 'invalid':\n",
        "            return penalty + self.reward['invalid']\n",
        "        elif mode == 'valid':\n",
        "            return (penalty + self.reward['valid']) * self.visited[self.human] ** 3\n",
        "        \n",
        "        \n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        (nrow, ncol), nmode = human, mode = self.state\n",
        "        #print ('HUman :' + str(human))\n",
        "\n",
        "        if self.maze[human] > 0.0:\n",
        "            self.visited[human] += 1  # mark visited cell\n",
        "        if human in self.targets:\n",
        "            self.targets.remove(human)\n",
        "        \n",
        "        valid_actions = self.valid_actions(human)\n",
        "        #print('Valid actions: ' +  str(valid_actions))\n",
        "            \n",
        "        #if not valid_actions: #doubtful\n",
        "        #    nmode = 'blocked'\n",
        "        if action in valid_actions:\n",
        "            nmode = 'valid'\n",
        "            if action == 0:\n",
        "                ncol -=1\n",
        "            elif action == 1:\n",
        "                nrow -=1\n",
        "            elif action == 2:\n",
        "                ncol += 1\n",
        "            elif action == 3:\n",
        "                nrow += 1\n",
        "        else:\n",
        "            nmode = 'blocked'\n",
        "        \n",
        "        human = (nrow, ncol)\n",
        "        self.human = human\n",
        "        self.state = (human, nmode)\n",
        "        \n",
        "        #print(self.state)\n",
        "        \n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        human, mode = self.state\n",
        "        if np.all(human == self.terminal_cell):\n",
        "            if len(self.targets) == 0:\n",
        "                return 'win'\n",
        "            else:\n",
        "                return 'lose'\n",
        "        return 'ongoing'\n",
        "    \n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        env_state = canvas.reshape((1, -1))\n",
        "        return env_state\n",
        "    \n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r,c] > 0.0:\n",
        "                    canvas[r,c] = 1.0\n",
        "        # draw the targets\n",
        "        for r,c in self.targets:\n",
        "            canvas[r,c] = target_mark\n",
        "        # draw the human\n",
        "        human, mode = self.state\n",
        "        #print(mode)\n",
        "        #print(human)\n",
        "        \n",
        "        canvas[human] = human_mark\n",
        "        #print(canvas)\n",
        "        return canvas\n",
        "        \n",
        "    def get_blocked_cells(self):\n",
        "        nrows, ncols = self._maze.shape\n",
        "        blocked_cells =set ((r,c) for r in range(nrows) for c in range(ncols) if maze1[r,c] == 0.0)\n",
        "        return blocked_cells\n",
        "    \n",
        "    def get_free_cells(self):\n",
        "        #    self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        free_cells = set((r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
        "        return free_cells\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AhfSbnu7NzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=100, discount=0.97):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        # episode = [env_state, action, reward, next_env_state, game_over]\n",
        "        # memory[i] = episode\n",
        "        # env_state == flattened 1d maze cells info, including agent cell (see method: observe)\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, env_state):\n",
        "        return self.model.predict(env_state)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]   # env_state 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        labels = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            env_state, action, reward, next_env_state, game_over = self.memory[j]\n",
        "            inputs[i] = env_state\n",
        "            # There should be no target values for actions not taken.\n",
        "            # Thou shalt not correct actions not taken #deep (quote by Eder Santana)\n",
        "            labels[i] = self.predict(env_state)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(next_env_state))\n",
        "            if game_over:\n",
        "                labels[i, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                labels[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmcyQTBt7PuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qtraining():\n",
        "    def __init__(self, model, env, **opt):\n",
        "        self.model = model #NN model\n",
        "        self.env = env     #Environment\n",
        "        self.n_epoch = opt.get('n_epoch', 1000)\n",
        "        self.max_memory = opt.get('max_memory', 4* self.env.maze.size)\n",
        "        self.data_size = opt.get('data_size', 0.75* self.env.maze.size)\n",
        "        self.start_cell = opt.get('start_cell', (0, 0))\n",
        "        self.init_hint = 4\n",
        "        self.weights_file = opt.get('weights_file', \"\")\n",
        "        self.name = opt.get('name', 'model')\n",
        "        self.win_count = 0\n",
        "        self.curr_epoch = 0\n",
        "        self.hint_list = []\n",
        "        self.item_collected = 0\n",
        "        \n",
        "        if self.weights_file:\n",
        "            print('Loading weights from file: %s' % (self.weights_file,))\n",
        "            self.model.load_weights(self.weights_file)\n",
        "            \n",
        "        ##if self.human_cells = 'all':\n",
        "        ##    self.human_cells = self.env.free_cells\n",
        "            \n",
        "        #Initialize experience replay object\n",
        "        self.experience = Experience(self.model, max_memory= self.max_memory)\n",
        "        \n",
        "    def train(self):\n",
        "        start_time = datetime.datetime.now()\n",
        "        self.seconds = 0\n",
        "        self.win_count = 0\n",
        "        self.curr_epoch = 0\n",
        "        self.hint_list = []\n",
        "        for epoch in range(self.n_epoch):\n",
        "            self.epoch = epoch\n",
        "            self.curr_epoch += 1\n",
        "            self.loss = 0.0\n",
        "            human = self.start_cell\n",
        "            #print(all_targets)\n",
        "            self.env.reset(self.start_cell, self.init_hint)\n",
        "            game_over = False\n",
        "            self.env_state = self.env.observe()\n",
        "            self.n_episodes = 0\n",
        "            self.hint_list = []\n",
        "            while not game_over:\n",
        "                game_over = self.play()\n",
        "\n",
        "            dt = datetime.datetime.now() - start_time\n",
        "            self.seconds = dt.total_seconds()\n",
        "            t = format_time(self.seconds)\n",
        "            fmt = \"Epoch: {:3d}/{:d} | Loss: {:.4f} | Episodes: {:4d} | Wins: {:2d} | Targets: {:d} | e: {:.3f} | time: {}\"\n",
        "            print(fmt.format(epoch, self.n_epoch-1, self.loss, self.n_episodes, self.win_count, len(self.env.targets), self.epsilon(), t))\n",
        "            print(str(env.total_reward) + str(env.targets) + str(env.state))\n",
        "            print('Hints :' +str(self.hint_list))\n",
        "\n",
        "            self.item_collected += (len(self.env._targets) - len(self.env.targets)) / 3\n",
        "\n",
        "            if self.win_count > 2:\n",
        "                if self.completion_check():\n",
        "                    print(\"Completed training at epoch: %d\" %(epoch, ))\n",
        "                    break     \n",
        "    def play(self):\n",
        "        hint = self.get_hint()\n",
        "        action = self.env.get_action(hint)\n",
        "        prev_env_state = self.env_state\n",
        "        self.env_state, reward, game_status = self.env.act(hint, action)\n",
        "        #print(game_status)\n",
        "        #print(env.total_reward)\n",
        "        if game_status == 'win':\n",
        "            self.win_count += 1\n",
        "            game_over = True\n",
        "        elif game_status == 'lose':\n",
        "            game_over = True\n",
        "        else:\n",
        "            game_over = False\n",
        "\n",
        "        #episode = [prev_env_sate, action, hint, reward, self.env_state, game_over]\n",
        "        episode = [prev_env_state, hint, reward, self.env_state, game_over]\n",
        "\n",
        "        self.experience.remember(episode)\n",
        "        self.n_episodes += 1\n",
        "\n",
        "        # Train model\n",
        "        inputs, targets = self.experience.get_data(data_size=self.data_size)\n",
        "        epochs = int(self.env.base)\n",
        "        h = self.model.fit(\n",
        "            inputs,\n",
        "            targets,\n",
        "            epochs = epochs,\n",
        "            batch_size=16,\n",
        "            verbose=0,\n",
        "        )\n",
        "        self.loss = self.model.evaluate(inputs, targets, verbose=0)\n",
        "        return game_over\n",
        "        \n",
        "    def run_game(self, human, hint):\n",
        "        self.env.reset(human, hint)\n",
        "        env_state = self.env.observe()\n",
        "        while True:\n",
        "            #get next hint\n",
        "            q = self.model.predict(env_state)\n",
        "            hint = np.argmax(q[0])\n",
        "            action = self.env.get_action(hint)\n",
        "            prev_env_state = env_state\n",
        "            #Apply hint\n",
        "            env_state, reward, game_status = self.env.act(hint, action)\n",
        "            if game_status == 'win':\n",
        "                return True\n",
        "            elif game_status == 'lose':\n",
        "                return False\n",
        "\n",
        "    def get_hint(self):\n",
        "        #Get next hint\n",
        "        valid_hints = self.env.valid_actions() + [4]\n",
        "        if np.random.rand()< self.epsilon():\n",
        "            hint = random.choice(valid_hints)\n",
        "        else:\n",
        "            q = self.experience.predict(self.env_state)\n",
        "            hint = np.argmax(q)\n",
        "        self.hint_list.append(hint)\n",
        "        return hint\n",
        "        \n",
        "    def epsilon(self):\n",
        "        #n = self.win_count\n",
        "        n = self.item_collected\n",
        "        high = 0.25\n",
        "        low = 0.05\n",
        "        e = low + (high - low) /  (1 + 0.01 * n**0.5 )\n",
        "        return e\n",
        "        \n",
        "    def completion_check(self):\n",
        "        if not self.run_game(self.start_cell):\n",
        "            return False\n",
        "        return True\n",
        "    \n",
        "    def save(self, name=\"\"):\n",
        "        # Save trained model weights and architecture, this will be used by the visualization code\n",
        "        if not name:\n",
        "            name = self.name\n",
        "        h5file = 'model_%s.h5' % (name,)\n",
        "        json_file = 'model_%s.json' % (name,)\n",
        "        self.model.save_weights(h5file, overwrite=True)\n",
        "        with open(json_file, \"w\") as outfile:\n",
        "            json.dump(self.model.to_json(), outfile)\n",
        "        t = format_time(self.seconds)\n",
        "        print('files: %s, %s' % (h5file, json_file))\n",
        "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (self.epoch, self.max_memory, self.data_size, t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POE_1syw7RNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(env, **opt):\n",
        "    loss = opt.get('loss', 'mse')\n",
        "    a = opt.get('alpha', 0.24)\n",
        "    model = Sequential()\n",
        "    esize = env.maze.size\n",
        "    model.add(Dense(esize, input_shape = (esize,)))\n",
        "    model.add(LeakyReLU(alpha = a))\n",
        "    model.add(Dense(esize))\n",
        "    model.add(LeakyReLU(alpha = a))\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(optimizer = 'adam', loss = 'mse')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3hgG77g7TBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_env(env, fname=None):\n",
        "    plt.grid('on')\n",
        "    n = env.maze.shape[0]\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, n, 1))\n",
        "    ax.set_yticks(np.arange(0.5, n, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(env.maze)\n",
        "    for cell in env.visited:\n",
        "        if env.visited[cell]:\n",
        "            canvas[cell] = visited_mark\n",
        "    for cell in env.targets:\n",
        "        canvas[cell] = target_mark\n",
        "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
        "    if fname:\n",
        "        plt.savefig(fname)\n",
        "    return img\n",
        "\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuqshTCj7VWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maze1 = np.asarray([\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
        "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
        "\n",
        "all_targets = [(0, 4), (2,3), (4,3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEfMwuMo7XIC",
        "colab_type": "code",
        "outputId": "82a6f4e0-55e6-4bed-bfdb-442da50a4e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "env = Maze(maze1, all_targets)\n",
        "show_env(env)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fce407a4dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stCOPzTz7YZh",
        "colab_type": "code",
        "outputId": "e42981c2-5cde-46e7-ff5c-2ae454b566ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model = build_model(env)\n",
        "\n",
        "qt = Qtraining(\n",
        "    model,\n",
        "    env,\n",
        "    n_epoch = 10000,\n",
        "    max_memory = 1000,\n",
        "    data_size = 256,\n",
        "    name = 'model_1'\n",
        ")\n",
        "\n",
        "qt.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0/9999 | Loss: 11.2878 | Episodes:   12 | Wins:  0 | Targets: 2 | e: 0.250 | time: 2.8 seconds\n",
            "-39.00952380952381{(2, 3), (4, 3)}((2, 6), 'blocked')\n",
            "Hints :[2, 2, 2, 2, 2, 3, 2, 3, 3, 4, 2, 3]\n",
            "Epoch:   1/9999 | Loss: 5.6941 | Episodes:   14 | Wins:  0 | Targets: 3 | e: 0.249 | time: 4.0 seconds\n",
            "-32.57959183673469{(2, 3), (4, 3), (0, 4)}((0, 1), 'blocked')\n",
            "Hints :[4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 1, 0, 1]\n",
            "Epoch:   2/9999 | Loss: 0.5636 | Episodes:   33 | Wins:  0 | Targets: 2 | e: 0.249 | time: 9.4 seconds\n",
            "-27.238095238095244{(2, 3), (4, 3)}((0, 5), 'valid')\n",
            "Hints :[4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 2, 3, 4, 4, 1, 4, 2, 2, 4, 4, 4, 4, 4, 1, 4, 4, 3, 1, 3, 4, 3, 1]\n",
            "Epoch:   3/9999 | Loss: 0.4301 | Episodes:   21 | Wins:  0 | Targets: 2 | e: 0.248 | time: 15.2 seconds\n",
            "-22.523809523809522{(4, 3), (0, 4)}((2, 5), 'valid')\n",
            "Hints :[4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4]\n",
            "Epoch:   4/9999 | Loss: 0.4068 | Episodes:   23 | Wins:  0 | Targets: 2 | e: 0.248 | time: 23.1 seconds\n",
            "-20.421768707482993{(2, 3), (4, 3)}((4, 5), 'valid')\n",
            "Hints :[4, 4, 4, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 3, 4, 4, 4, 4, 0, 4, 4, 3, 1]\n",
            "Epoch:   5/9999 | Loss: 0.7332 | Episodes:   49 | Wins:  0 | Targets: 1 | e: 0.248 | time: 45.3 seconds\n",
            "-37.04761904761905{(0, 4)}((4, 5), 'valid')\n",
            "Hints :[4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 0, 3, 3, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}